import os
import csv
import numpy as np
import matplotlib.pyplot as plt
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv
from stable_baselines3.common.callbacks import BaseCallback
from compost_env_kinetic_physical import CompostEnvKineticPhysical


# ========== è‡ªå®šä¹‰å›è°ƒå‡½æ•°ï¼šåŸºäºç»¼åˆæ§åˆ¶æŒ‡æ ‡åˆ¤æ–­æœ€ä¼˜æ¨¡å‹ ==========
class CustomEvalCallback(BaseCallback):
    """
    è‡ªå®šä¹‰æ¨¡å‹è¯„ä¼°å›è°ƒï¼Œç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ ¹æ®å‡æ¸©é€Ÿåº¦ã€é«˜æ¸©æŒç»­æ—¶é•¿ã€æ°§æ°”æ§åˆ¶å’ŒèŠ‚èƒ½æƒ…å†µç»¼åˆè¯„ä¼°æ¨¡å‹ï¼Œ
    ä¿å­˜è¡¨ç°æœ€å¥½çš„æ¨¡å‹ã€‚
    """
    def __init__(self, eval_env, eval_freq, save_path, verbose=1):
        super().__init__(verbose)
        self.eval_env = eval_env
        self.eval_freq = eval_freq
        self.save_path = save_path
        self.best_score = -np.inf
        os.makedirs(save_path, exist_ok=True)

    def _on_step(self):
        # æ¯ eval_freq æ­¥è¯„ä¼°ä¸€æ¬¡
        if self.n_calls % self.eval_freq == 0:
            score = self.evaluate_policy()
            if score > self.best_score:
                self.best_score = score
                model_name = f"custom_best_step{self.num_timesteps}_score{score:.1f}.zip"
                self.model.save(os.path.join(self.save_path, model_name))
                self.model.save(os.path.join(self.save_path, "best_model.zip"))
                print(f"\nâœ… æ–°æœ€ä¼˜æ¨¡å‹ä¿å­˜: {model_name}\n")
        return True

    def evaluate_policy(self):
        """
        ç­–ç•¥è¯„ä¼°å‡½æ•°ï¼šæ¨¡æ‹Ÿä¸€æ¬¡å®Œæ•´å †è‚¥è¿‡ç¨‹ï¼Œè®¡ç®—ç»¼åˆè¯„åˆ†ã€‚
        """
        obs, _ = self.eval_env.reset()
        done = False
        step = 0

        # å„é¡¹æŒ‡æ ‡åˆå§‹åŒ–
        heating_achieved_step = None       # å‡æ¸©è‡³55Â°Cçš„é¦–æ¬¡æ—¶é—´
        high_temp_duration = 0             # é«˜æ¸©é˜¶æ®µæŒç»­æ—¶é—´
        total_air_on = 0                   # é€šé£æ€»æ¬¡æ•°
        O2_min = 100                       # æœ€ä½æ°§æ°”
        O2_good_steps = 0                 # O2 åœ¨16~19% åŒºé—´æ­¥æ•°
        O2_too_high_steps = 0             # O2 > 19% æ­¥æ•°
        T_reached_55 = False

        while not done:
            action, _ = self.model.predict(obs, deterministic=True)
            obs, _, terminated, truncated, _ = self.eval_env.step(action)
            done = terminated or truncated
            T_avg = np.mean(obs[:3])
            O2 = obs[3]
            step = int(obs[7])

            # åˆ¤æ–­æ˜¯å¦åœ¨å‰48å°æ—¶å‡è‡³ 55Â°C
            if not T_reached_55 and T_avg >= 55:
                T_reached_55 = True
                if step <= 288:
                    heating_achieved_step = step

            # ä¸€æ—¦æ¸©åº¦è¾¾åˆ° 55Â°Cï¼Œå³è¿›å…¥é«˜æ¸©åˆ¤å®šé˜¶æ®µ
            if T_avg >= 55 and T_avg <= 70:
                high_temp_duration += 1

            # æ°”ä½“æ§åˆ¶
            O2_min = min(O2_min, O2)
            if 16 <= O2 <= 19:
                O2_good_steps += 1
            if O2 > 19:
                O2_too_high_steps += 1

            # é€šé£æ¬¡æ•°
            if action == 1:
                total_air_on += 1

        # -------- ç»¼åˆè¯„åˆ†è®¡ç®— --------
        # 1. å‡æ¸©æœŸå¾—åˆ†ï¼šè¶Šæ—©å‡æ¸©è¶Šé«˜åˆ†ï¼ˆæ»¡åˆ† 50ï¼‰
        if heating_achieved_step:
            heating_score = (1 - heating_achieved_step / 288) * 50
        else:
            heating_score = -100  # æ²¡æœ‰å‡æ¸©

        # 2. é«˜æ¸©æŒç»­å¾—åˆ†ï¼šæ»¡åˆ† 100
        high_temp_score = (high_temp_duration / (self.eval_env.total_steps - 288)) * 100

        # 3. æ°§æ°”å¾—åˆ†ï¼ˆO2 ä¸ä½äº15%ï¼Œ16~19åŒºé—´æ­¥æ•°å æ¯”é«˜ï¼‰
        if O2_min < 15:
            o2_score = -1000  # ä¸¥é‡ç¼ºæ°§
        else:
            o2_score = (O2_good_steps / self.eval_env.total_steps) * 30
            o2_score -= (O2_too_high_steps / self.eval_env.total_steps) * 100  # O2 > 19 æƒ©ç½š

        # 4. èŠ‚èƒ½å¾—åˆ†ï¼šé€šé£è¶Šå°‘è¶Šé«˜ï¼Œæœ€å¤š840æ­¥ï¼Œæ»¡åˆ†10
        energy_score = max(0, (1 - total_air_on / 840) * 10)

        # 5. æ€»åˆ†
        total_score = heating_score + high_temp_score + o2_score + energy_score
        return total_score


# ========== ä¸»ç¨‹åºå…¥å£ ==========
def main():
    # åˆ›å»ºæ—¥å¿—ä¸æ¨¡å‹ä¿å­˜ç›®å½•
    log_dir = "logs/"
    best_model_dir = "./best_model"
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(best_model_dir, exist_ok=True)

    # åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒä¸è¯„ä¼°ç¯å¢ƒ
    num_cpu = 9
    env = SubprocVecEnv([lambda: CompostEnvKineticPhysical() for _ in range(num_cpu)])
    eval_env = CompostEnvKineticPhysical()

    # åˆå§‹åŒ– PPO æ¨¡å‹
    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        tensorboard_log=log_dir,
        learning_rate=2e-4,
        n_steps=2048,
        batch_size=256,
        n_epochs=10,
        gamma=0.99,
        ent_coef=0.3,
        device="cpu"
    )

    # å®šä¹‰å›è°ƒå‡½æ•°ï¼šæŒ‰æŒ‡æ ‡ä¿å­˜æœ€ä¼˜æ¨¡å‹
    eval_callback = CustomEvalCallback(
        eval_env=eval_env,
        eval_freq=5000,
        save_path=best_model_dir
    )

    # å¼€å§‹è®­ç»ƒ
    model.learn(total_timesteps=10_000_000, callback=eval_callback)

    # ===== è®­ç»ƒç»“æŸåè¯„ä¼°æœ€ä¼˜æ¨¡å‹ =====
    print("\nğŸ“Š è®­ç»ƒå®Œæˆï¼ŒåŠ è½½æœ€ä½³æ¨¡å‹å¹¶ç»˜å›¾...\n")
    best_model = PPO.load(os.path.join(best_model_dir, "best_model.zip"))
    obs, _ = eval_env.reset()

    # åˆå§‹åŒ–ç»“æœåˆ—è¡¨
    T1_list, T2_list, T3_list = [], [], []
    CO2_list, O2_list, H2O_list, actions, steps, rewards = [], [], [], [], [], []
    done = False
    total_reward = 0

    while not done:
        action, _ = best_model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, _ = eval_env.step(action)
        done = terminated or truncated
        step = int(obs[7])
        T1_list.append(obs[0])
        T2_list.append(obs[1])
        T3_list.append(obs[2])
        O2_list.append(obs[3])
        CO2_list.append(obs[4])
        H2O_list.append(obs[5])
        actions.append(int(action))
        steps.append(step)
        rewards.append(reward)
        total_reward += reward

    # ä¿å­˜CSVç»“æœæ–‡ä»¶
    with open("run_result_final.csv", "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["Hour", "T1", "T2", "T3", "O2", "CO2", "H2O", "Action", "Reward"])
        for i in range(len(T1_list)):
            hour = float(steps[i]) * 10 / 60
            writer.writerow([hour, T1_list[i], T2_list[i], T3_list[i],
                             O2_list[i], CO2_list[i], H2O_list[i], actions[i], rewards[i]])

    # ç»“æœå¯è§†åŒ–ç»˜å›¾
    T_avg = np.mean([T1_list, T2_list, T3_list], axis=0)
    time = [s * 10 / 60 for s in steps]
    plt.figure(figsize=(12, 12))

    plt.subplot(6, 1, 1)
    plt.plot(time, T_avg, label="Avg Temperature (Â°C)", color="steelblue")
    plt.axhline(55, linestyle="--", color="gray", label="Target 55Â°C")
    plt.ylabel("T avg (Â°C)")
    plt.legend()

    plt.subplot(6, 1, 2)
    plt.plot(time, O2_list, label="Oxygen (%)", color="green")
    plt.axhline(18, linestyle="--", color="red", label="Oâ‚‚ Warning")
    plt.axhline(15, linestyle="--", color="darkred", label="Oâ‚‚ Critical")
    plt.ylabel("Oâ‚‚ (%)")
    plt.legend()

    plt.subplot(6, 1, 3)
    plt.plot(time, CO2_list, label="COâ‚‚ (%)", color="orange")
    plt.axhline(1.0, linestyle="--", color="gray")
    plt.ylabel("COâ‚‚ (%)")
    plt.legend()

    plt.subplot(6, 1, 4)
    plt.plot(time, H2O_list, label="Moisture (%)", color="blue")
    plt.axhline(0.6, linestyle="--", color="gray")
    plt.ylabel("Hâ‚‚O (%)")
    plt.legend()

    plt.subplot(6, 1, 5)
    plt.step(time, actions, label="Action (0=off, 1=on)", color="black", where='post')
    plt.ylabel("Action")
    plt.legend()

    plt.subplot(6, 1, 6)
    plt.plot(time, rewards, label="Reward", color="purple")
    plt.ylabel(f"Reward\nTotal={total_reward:.1f}")
    plt.xlabel("Time (hour)")
    plt.legend()

    plt.tight_layout()
    plt.savefig("run_result_final.png", dpi=300)
    plt.show()


if __name__ == "__main__":
    main()
